<!DOCTYPE html>
<html lang="sv">
  <head>
    <meta charset="utf-8">
    <title>Index</title>
    <link rel="stylesheet" href="test.css">
    <meta name="viewport" content="width-device-width, initial scale=1">
  </head>
  <body>
    <div class="container">
      <div class="itema"> <ul id="ul">
        <li><a href="Index.html">Index</a></li>
        <li><a href="Etik.html">Etik</a></li>
        <li><a href="file:///C:/Users/Kalle/AppData/Local/atom/Historia.html">Historia</a></li>
        <li><a href="Hot.html">Hot</a></li>
        <li><a href="">Om oss</a></li>
        </ul>
    </div>
    <div class="itemb"> <ul id="ul">
      </div>

      <div class="itemc">
<header>
  <h1>Etik</h1>
  </header>
  <section>
    <iframe width="420" height="345"
    src="https://www.youtube.com/watch?v=pdsyNwUoON0">
  </iframe>
  </section>
<main>
  Robot ethics
  Main article: Robot ethics

  The term "robot ethics" (sometimes "roboethics") refers to the morality of how humans design, construct, use and treat robots.[2] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software.[3] Not all robots functions through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.
  Machine ethics
  Main article: Machine ethics
<h2>Mer om etik</h2>
  Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[4][5][6][7] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[8]

  Isaac Asimov considered the issue in the 1950s in his I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.[9] More recently, academics and many governments have challenged the idea that AI can itself be held accountable.[10] A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.[11]

  In 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne in Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.[12]

  Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[13] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[14][15] The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue.[16] They point to programs like the Language Acquisition Device which can emulate human interaction.

  Vernor Vinge has suggested that a moment may come when some computers are smarter than humans. He calls this "the Singularity."[17] He suggests that it may be somewhat or possibly very dangerous for humans.[18] This is discussed by a philosophy called Singularitarianism. The Machine Intelligence Research Institute has suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[19]

  There are discussion on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[20] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI’s decision is ethical or unethical.[20]

  In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[17]

  However, there is one technology in particular that could truly bring the possibility of robots with moral competence to reality. In a paper on the acquisition of moral values by robots, Nayef Al-Rodhan mentions the case of neuromorphic chips, which aim to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[21] Robots embedded with neuromorphic technology could learn and develop knowledge in a uniquely humanlike way. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, a pro-survival attitude, hesitation etc.

  In Moral Machines: Teaching Robots Right from Wrong,[22] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. Nick Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3) over neural networks and genetic algorithms on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. stare decisis),[23] while Chris Santos-Lang argued in the opposite direction on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal "hackers".[24]

  According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that don't require a human controller.[25]
</main>



<footer>
  All rights reserved Kalle Fogde Stille
  </footer>
        </div>
        <div class="itemd"> <ul id="ul">
      </div>
  </body>
</html>
